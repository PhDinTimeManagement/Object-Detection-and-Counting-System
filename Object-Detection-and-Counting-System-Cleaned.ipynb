{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcaa9f7d949342c1",
   "metadata": {
    "id": "fcaa9f7d949342c1"
   },
   "source": [
    "# Dataset Import"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install roboflow",
   "id": "5363af4eca2cf960",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c943357938c6029d",
   "metadata": {
    "id": "c943357938c6029d",
    "outputId": "4d650c75-97d3-4c22-a597-b8fe504ef8ff"
   },
   "source": [
    "from roboflow import Roboflow\n",
    "\n",
    "rf = Roboflow(api_key=\"blocked\")\n",
    "workspaces = rf.workspace()\n",
    "print(workspaces)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f89e68a9ddba7514",
   "metadata": {
    "id": "f89e68a9ddba7514",
    "outputId": "ef54e968-3a9c-4b4b-a074-30b1976021df"
   },
   "source": [
    "workspace = rf.workspace(\"hkpolyu-comp4423-group-project\")\n",
    "project = workspace.project(\"canteenq-counter\")\n",
    "VERSION = 7\n",
    "dataset_version = project.version(VERSION)\n",
    "dataset = dataset_version.download(\"yolov8\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "38bb0eeb9431760f",
   "metadata": {
    "id": "38bb0eeb9431760f"
   },
   "source": [
    "import os\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "dataset_location = f\"{HOME}/CanteenQ-Counter-{VERSION}/data.yaml\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7b8bfddf5e9e99d",
   "metadata": {
    "id": "7b8bfddf5e9e99d"
   },
   "source": [
    "# Segmentation using DeepLab v3+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abdfda8efa69c79",
   "metadata": {
    "id": "8abdfda8efa69c79"
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "b58cf10cf9c611f",
   "metadata": {
    "id": "b58cf10cf9c611f",
    "outputId": "644ba146-0d8a-4f1a-c681-759c51b7536b"
   },
   "source": [
    "base_dir = './DeepLabV3Plus_Project'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "os.chdir(base_dir)\n",
    "\n",
    "!git clone https://github.com/VainF/DeepLabV3Plus-Pytorch\n",
    "\n",
    "os.chdir('DeepLabV3Plus-Pytorch')\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "checkpoints_path = './checkpoints/all.zip'\n",
    "if not os.path.isfile(checkpoints_path):\n",
    "    !wget -O {checkpoints_path} https://www.dropbox.com/sh/w3z9z8lqpi8b2w7/AAB0vkl4F5vy6HdIhmRCTKHSa?dl=1\n",
    "    !unzip {checkpoints_path} -d ./checkpoints/"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6e2f966e",
   "metadata": {
    "id": "6e2f966e",
    "outputId": "c4d290b1-f3f2-4dfc-d6d2-6f005c90a245"
   },
   "source": [
    "!python predict.py --input ../../Test_Image_Folder --model deeplabv3plus_mobilenet --ckpt checkpoints/best_deeplabv3plus_mobilenet_voc_os16.pth --save_val_results_to test_results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a4729776b509b96d",
   "metadata": {
    "id": "a4729776b509b96d"
   },
   "source": [
    "## Post-processing of the model's output"
   ]
  },
  {
   "cell_type": "code",
   "id": "664c138f8a5bfeed",
   "metadata": {
    "id": "664c138f8a5bfeed"
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "def detect_people(image):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    mask = cv2.inRange(hsv_image, np.array([0, 50, 50]), np.array([10, 255, 255]))\n",
    "    mask_inv = cv2.bitwise_not(mask)\n",
    "    image[mask_inv == 255] = [0, 0, 0]\n",
    "\n",
    "    kernel = np.ones((7, 7), np.uint8)\n",
    "    eroded_mask = cv2.erode(mask, kernel, iterations=2)\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(eroded_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    people_contours = [contour for contour in contours if\n",
    "                       cv2.contourArea(contour) > 100 and cv2.contourArea(contour) / cv2.arcLength(contour, True) > 0.5]\n",
    "\n",
    "    return people_contours, image"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c29826d014a132cb",
   "metadata": {
    "id": "c29826d014a132cb",
    "outputId": "c57f8dfa-032a-417d-82f7-0c709d4ea8dc"
   },
   "source": [
    "import os\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def test_on_img_detect_people(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return\n",
    "\n",
    "    people_contours, image = detect_people(image)\n",
    "    people_count = len(people_contours)\n",
    "    for contour in people_contours:\n",
    "        cv2.drawContours(image, [contour], -1, (0, 255, 0), 2)\n",
    "\n",
    "    print(f\"Number of Humans in {image_path}: {people_count}\")\n",
    "    resized_image = cv2.resize(image, (0, 0), fx=0.2, fy=0.2)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "\n",
    "folder_path=f\"{HOME}/DeepLabV3Plus_Project/DeepLabV3Plus-Pytorch/test_results\"\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".png\"):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        test_on_img_detect_people(image_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ee2ad4afa8e6dfed",
   "metadata": {
    "id": "ee2ad4afa8e6dfed"
   },
   "source": [
    "## Saved images"
   ]
  },
  {
   "cell_type": "code",
   "id": "15345ee2",
   "metadata": {
    "id": "15345ee2"
   },
   "source": [
    "def predict_list_img(frameList, frameCountList):\n",
    "    cwd = os.getcwd()\n",
    "    save_path = os.path.join(cwd, \"images\")\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    os.chdir(save_path) # Change directory to /images\n",
    "\n",
    "    # Save current images in the cwd/image\n",
    "    for frame, count in zip(frameList, frameCountList):\n",
    "        cv2.imwrite(f\"{save_path}/{count}.jpg\", frame)\n",
    "\n",
    "    os.chdir(cwd) # Change directory back to the model's path\n",
    "    !python predict.py --input {save_path} --model deeplabv3plus_mobilenet --ckpt checkpoints/best_deeplabv3plus_mobilenet_voc_os16.pth --save_val_results_to test_results\n",
    "\n",
    "# Load the model's output picture from package test_results\n",
    "def load_img_draw(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return\n",
    "\n",
    "    people_contours, image = detect_people(image)\n",
    "    people_count = len(people_contours)\n",
    "    for contour in people_contours:\n",
    "        cv2.drawContours(image, [contour], -1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.putText(image, f\"Number of Humans: {people_count}\", (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    return image"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "266f982a6d3b1e6a",
   "metadata": {
    "id": "266f982a6d3b1e6a"
   },
   "source": [
    "## Video Object Detection Counter"
   ]
  },
  {
   "cell_type": "code",
   "id": "94245954",
   "metadata": {
    "id": "94245954",
    "outputId": "43c44b4f-16ff-4f87-af95-38db3215ef29"
   },
   "source": [
    "# In progress\n",
    "video_file_path = f\"{HOME}/VSQ_4.30_OrgVideo.mov\"\n",
    "cap = cv2.VideoCapture(video_file_path)\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "# Get video properties\n",
    "w, h, fps = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)), cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Number of frames to skip\n",
    "# Speed up video 2 times - Set 0.05\n",
    "skip_frames = int(0.1 * fps)\n",
    "\n",
    "# Video writer\n",
    "video_writer = cv2.VideoWriter(\n",
    "    \"Segmentation_VSQ_Obeject_Counter_4.30.mp4\",\n",
    "    cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "    fps,\n",
    "    (w, h)\n",
    ")\n",
    "\n",
    "frameList = [] # Frame object, type: frame\n",
    "frameCountList = [] # Frame count, type: int\n",
    "frame_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Skip frames if not true\n",
    "    if frame_count % skip_frames == 0:\n",
    "        # Add the frame that need to be processed into the model;\n",
    "        # record the frame count\n",
    "        frameList.append(frame)\n",
    "        frameCountList.append(frame_count)\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "\n",
    "predict_list_img(frameList, frameCountList) # Save the images that have be predicted.\n",
    "\n",
    "video_file_path = f\"{HOME}/VSQ_4.30_OrgVideo.mov\"\n",
    "cap = cv2.VideoCapture(video_file_path)\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "save_path = os.getcwd()\n",
    "save_path = os.path.join(save_path, \"test_results\")\n",
    "\n",
    "frame_count = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if frame_count % skip_frames == 0:\n",
    "        # Frame that need to be processed\n",
    "        # Frame proceesing\n",
    "        img_path = f\"{save_path}/{frame_count}.png\"\n",
    "        processed_frame = load_img_draw(img_path)\n",
    "        video_writer.write(processed_frame)\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "# Release everything when the job is finished\n",
    "cap.release()\n",
    "video_writer.release()  # The video's ouput path is released under \"DeepLabV3Plus-Pytorch\" package\n",
    "# cv2.destroyAllWindows()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4a405eda",
   "metadata": {
    "id": "4a405eda"
   },
   "source": [
    "# Yolov8 Object Detection Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c9fc27da766ee5",
   "metadata": {
    "id": "f5c9fc27da766ee5"
   },
   "source": [
    "## Installation of the Yolov8 model"
   ]
  },
  {
   "cell_type": "code",
   "id": "f7c70b9e",
   "metadata": {
    "id": "f7c70b9e",
    "outputId": "03ff1e43-2d1f-4ecb-e8a6-bf34f5d36beb"
   },
   "source": [
    "!pip install ultralytics yolov8"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d3f8a64ea854972b",
   "metadata": {
    "id": "d3f8a64ea854972b",
    "outputId": "45e2efb0-5b5b-4813-9f87-4f58ab72d664"
   },
   "source": [
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e1e473f6",
   "metadata": {
    "id": "e1e473f6"
   },
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "id": "28258664",
   "metadata": {
    "id": "28258664"
   },
   "source": [
    "os.chdir(HOME)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a54e0014",
   "metadata": {
    "id": "a54e0014"
   },
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8n.pt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "644c70c1",
   "metadata": {
    "scrolled": true,
    "id": "644c70c1",
    "outputId": "06601662-c527-4454-9705-9b8d39efe4e9"
   },
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "model.tune(data = f\"{dataset_location}\", epochs = 10, iterations = 100, optimizer = 'AdamW',plots = False, save = False, val = False, lr0 = 0.01027, lrf = 0.01033, momentum=0.93038, weight_decay=0.00051, warmup_epochs=2.89715, warmup_momentum= 0.60719, box=7.45953, cls=0.44658, dfl=1.39703, hsv_h= 0.01204, hsv_s=0.7691, hsv_v=0.46525, translate=0.09319, scale=0.40937, fliplr=0.5084, mosaic=0.83653)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "33ed1f94",
   "metadata": {
    "id": "33ed1f94",
    "outputId": "dbb2178e-ef87-4e46-8181-3c633c94e33a"
   },
   "source": [
    "# Showcase Tuning Result\n",
    "tuneDir = \"tune2\"\n",
    "files_dict = {}\n",
    "\n",
    "tune_files = os.listdir(f\"{HOME}/runs/detect/{tuneDir}\")\n",
    "files_dict[tuneDir] = tune_files\n",
    "\n",
    "for dir_name, documents in files_dict.items():\n",
    "    print(f\"File Name: {dir_name}\")\n",
    "    for doc in documents:\n",
    "        print(f\"Document Name: {doc}\")\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2ba81c0a",
   "metadata": {
    "scrolled": false,
    "id": "2ba81c0a",
    "outputId": "1e2b6bdd-3c56-4a2f-f7f9-48ea1f1502bd"
   },
   "source": [
    "from IPython.display import display\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def is_image(file_name):\n",
    "    return file_name.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "\n",
    "for dir_name in [tuneDir, trainDir]:\n",
    "    path = f\"{HOME}/runs/detect/{dir_name}\"\n",
    "    files = os.listdir(path)\n",
    "    img_files = [file for file in files if is_image(file)]\n",
    "\n",
    "    for img in img_files:\n",
    "        img_path = os.path.join(path, img)\n",
    "        res_img = Image.open(img_path)\n",
    "\n",
    "        if res_img.mode == \"RGBA\":\n",
    "            res_img = res_img.convert(\"RGB\")\n",
    "        print(f\"Image path: {path}/{img}\")\n",
    "        display(res_img)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "93bc78a2",
   "metadata": {
    "id": "93bc78a2"
   },
   "source": [
    "## Yolov8 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "id": "71681d20f2319f22",
   "metadata": {
    "scrolled": true,
    "id": "71681d20f2319f22",
    "outputId": "ad08707d-f2df-42be-fd4e-72e381deea2f"
   },
   "source": [
    "# Use the model\n",
    "model.train(data=f\"{dataset_location}\", epochs=100, lr0=0.00622, lrf=0.00618, momentum=0.80774, weight_decay=0.00056, warmup_epochs=1.8395, warmup_momentum=0.52433, box=8.45512, cls=0.37275, dfl=1.18532, hsv_h=0.01438, hsv_s=0.75444, hsv_v=0.37201, translate=0.09335, scale=0.29729, fliplr=0.38751, mosaic=0.92436)\n",
    "metrics = model.val()  # evaluate model performance on the validation set"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a3e846d3fc529a59",
   "metadata": {
    "id": "a3e846d3fc529a59",
    "outputId": "5c4ace90-f189-4e7e-e292-f7433b608af7"
   },
   "source": [
    "# Showcase Training Result\n",
    "\n",
    "# Need to change to best hyperparameter train\n",
    "trainDir = \"train103\"\n",
    "files_dict = {}\n",
    "\n",
    "train_files = os.listdir(f\"{HOME}/runs/detect/{trainDir}\")\n",
    "files_dict[trainDir] = train_files\n",
    "\n",
    "for dir_name, documents in files_dict.items():\n",
    "    print(f\"File Name: {dir_name}\")\n",
    "    for doc in documents:\n",
    "        print(f\"Document Name: {doc}\")\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee2f24cccca888f8",
   "metadata": {
    "scrolled": true,
    "id": "ee2f24cccca888f8",
    "outputId": "f76ed6ca-64c6-4ad3-8b06-6bea9c822e26"
   },
   "source": [
    "from IPython.display import display\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def is_image(file_name):\n",
    "    return file_name.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "\n",
    "for dir_name in [trainDir]:\n",
    "    path = f\"{HOME}/runs/detect/{dir_name}\"\n",
    "    files = os.listdir(path)\n",
    "    img_files = [file for file in files if is_image(file)]\n",
    "\n",
    "    for img in img_files:\n",
    "        img_path = os.path.join(path, img)\n",
    "        res_img = Image.open(img_path)\n",
    "\n",
    "        if res_img.mode == \"RGBA\":\n",
    "            res_img = res_img.convert(\"RGB\")\n",
    "        print(f\"Image path: {path}/{img}\")\n",
    "        display(res_img)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "be8b091d",
   "metadata": {
    "id": "be8b091d"
   },
   "source": [
    "## CanteenQ Object Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a9b6f",
   "metadata": {
    "id": "be0a9b6f"
   },
   "source": [
    "## Image Testing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "outputId": "31d6f670-95a7-4c8d-e9b5-82d11e566a3b",
    "id": "fT5SX8ORYmRD"
   },
   "source": [
    "img_path = f\"{HOME}/Test_Image_Folder/VS_150.jpeg\"\n",
    "results = model(img_path)\n",
    "for result in results:\n",
    "    boxes = result.boxes\n",
    "    probs = result.probs\n",
    "    result.save(filename=\"testWithAImg.jpeg\")\n",
    "\n",
    "saved_img_path = f\"{HOME}/testWithAImg.jpeg\"\n",
    "saved_img = Image.open(saved_img_path)\n",
    "display(saved_img)"
   ],
   "id": "fT5SX8ORYmRD",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f43bd156c837944a",
   "metadata": {
    "id": "f43bd156c837944a",
    "outputId": "bfb57d38-7b70-4fab-be7a-baf44637b134"
   },
   "source": [
    "import cv2\n",
    "from statistics import mode\n",
    "\n",
    "cap = cv2.VideoCapture(f\"{HOME}/VSQ_4.30_OrgVideo.mov\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "# Get video properties\n",
    "w, h, fps = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)), cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Number of frames to skip\n",
    "# Speed up video 2 times - Set 0.05\n",
    "skip_frames = int(0.05 * fps)\n",
    "\n",
    "# Video writer\n",
    "video_writer = cv2.VideoWriter(\n",
    "    \"YOLOv8_VSQ_Object_Counter_Res.mp4\",\n",
    "    cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "    fps,\n",
    "    (w, h)\n",
    ")\n",
    "\n",
    "frame_count = 0\n",
    "kiosks_array = []\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Skip frames\n",
    "    if frame_count % skip_frames == 0:\n",
    "        results = model(frame)\n",
    "        for result in results:\n",
    "            num_detections = result.boxes.cls.numel()\n",
    "            num_humans = 0\n",
    "            num_orderKiosks = 0\n",
    "            for det in range(num_detections):\n",
    "                class_id = int(result.boxes.cls[det].item())\n",
    "                x1, y1, x2, y2 = result.boxes.xyxy[det].tolist()\n",
    "\n",
    "                if class_id == 0:\n",
    "                    num_humans += 1\n",
    "                    color = (0, 255, 0)  # Green for humans\n",
    "                else:\n",
    "                    num_orderKiosks += 1\n",
    "                    color = (0, 0, 255)  # Red for kiosks\n",
    "\n",
    "                # Draw bounding boxes using the xyxy attribute\n",
    "                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "\n",
    "            kiosks_array.append(num_orderKiosks)\n",
    "            # Calculate average wait time\n",
    "            unit = 0.6\n",
    "            if num_orderKiosks == 0:\n",
    "                num_orderKiosks = mode(kiosks_array)\n",
    "            avg_waitT = round(num_humans * unit / num_orderKiosks, 3)\n",
    "\n",
    "            cv2.putText(frame, f\"Number of Humans: {num_humans}\", (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"Number of Ordering Kiosks: {num_orderKiosks}\", (20, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.putText(frame, f\"Average Waiting Time (in Minutes): {avg_waitT}\", (20, 180), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "        video_writer.write(frame)\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "# Release everything when the job is finished\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "04740b53",
   "metadata": {
    "id": "04740b53"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
